{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special IPython command to prepare the notebook for matplotlib\n",
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "# special matplotlib argument for improved plots\n",
    "from matplotlib import rcParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "# Generic classification and optimization functions from last lab\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "# clf - original classifier\n",
    "# parameters - grid to search over\n",
    "# X - usually your training X matrix\n",
    "# y - usually your training y \n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None):\n",
    "    if score_func:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n",
    "    else:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n",
    "    gs.fit(X, y)\n",
    "    print \"BEST\", gs.best_params_, gs.best_score_, gs.grid_scores_\n",
    "    best = gs.best_estimator_\n",
    "    return best\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "# Important parameters\n",
    "# indf - Input dataframe\n",
    "# featurenames - vector of names of predictors\n",
    "# targetname - name of column you want to predict (e.g. 0 or 1, 'M' or 'F', \n",
    "#              'yes' or 'no')\n",
    "# target1val - particular value you want to have as a 1 in the target\n",
    "# mask - boolean vector indicating test set (~mask is training set)\n",
    "# reuse_split - dictionary that contains traning and testing dataframes \n",
    "#              (we'll use this to test different classifiers on the same \n",
    "#              test-train splits)\n",
    "# score_func - we've used the accuracy as a way of scoring algorithms but \n",
    "#              this can be more general later on\n",
    "# n_folds - Number of folds for cross validation ()\n",
    "# n_jobs - used for parallelization\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "\n",
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, mask=None, reuse_split=None, score_func=None, n_folds=5, n_jobs=1):\n",
    "    subdf=indf[featurenames]\n",
    "    X=subdf.values\n",
    "    y=(indf[targetname].values==target1val)*1\n",
    "    if mask !=None:\n",
    "        print \"using mask\"\n",
    "        Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n",
    "    if reuse_split !=None:\n",
    "        print \"using reuse split\"\n",
    "        Xtrain, Xtest, ytrain, ytest = reuse_split['Xtrain'], reuse_split['Xtest'], reuse_split['ytrain'], reuse_split['ytest']\n",
    "    if parameters:\n",
    "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_jobs=n_jobs, n_folds=n_folds, score_func=score_func)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print \"############# based on standard predict ################\"\n",
    "    print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "    print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
    "    print confusion_matrix(ytest, clf.predict(Xtest))\n",
    "    print \"########################################################\"\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "# Plot tree containing only two covariates\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "# cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "def plot_2tree(ax, Xtr, Xte, ytr, yte, clf, plot_train = True, plot_test = True, lab = ['Feature 1', 'Feature 2'], mesh=True, colorscale=cmap_light, cdiscrete=cmap_bold, alpha=0.3, psize=10, zfunc=False):\n",
    "    # Create a meshgrid as our test data\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plot_step= 0.05\n",
    "    xmin, xmax= Xtr[:,0].min(), Xtr[:,0].max()\n",
    "    ymin, ymax= Xtr[:,1].min(), Xtr[:,1].max()\n",
    "    xx, yy = np.meshgrid(np.arange(xmin, xmax, plot_step), np.arange(ymin, ymax, plot_step) )\n",
    "\n",
    "    # Re-cast every coordinate in the meshgrid as a 2D point\n",
    "    Xplot= np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "\n",
    "    # Predict the class\n",
    "    Z = clfTree1.predict( Xplot )\n",
    "\n",
    "    # Re-shape the results\n",
    "    Z= Z.reshape( xx.shape )\n",
    "    cs = plt.contourf(xx, yy, Z, cmap= cmap_light, alpha=0.3)\n",
    "  \n",
    "    # Overlay training samples\n",
    "    if (plot_train == True):\n",
    "        plt.scatter(Xtr[:, 0], Xtr[:, 1], c=ytr-1, cmap=cmap_bold, alpha=alpha,edgecolor=\"k\") \n",
    "    # and testing points\n",
    "    if (plot_test == True):\n",
    "        plt.scatter(Xte[:, 0], Xte[:, 1], c=yte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\")\n",
    "\n",
    "    plt.xlabel(lab[0])\n",
    "    plt.ylabel(lab[1])\n",
    "    plt.title(\"Boundary for decision tree classifier\",fontsize=7.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 10.,   0.,  53.,   0., 681.,   0., 638.,   0., 199.,  18.]),\n",
       " array([3. , 3.5, 4. , 4.5, 5. , 5.5, 6. , 6.5, 7. , 7.5, 8. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "\n",
    "plt.hist(df.quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sravya bhaskara\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "Y = df['quality'].values\n",
    "df_tmp = df.drop('quality',1)\n",
    "Y = np.array([1 if y>=7 else 0 for y in Y])\n",
    "X = df_tmp.as_matrix()\n",
    "\n",
    "df['target'] = (df['quality'].values >=7)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnames = list(df.columns.values[1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1357098186366479"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test/train mask\n",
    "itrain, itest = train_test_split(xrange(df.shape[0]), train_size=0.6)\n",
    "mask=np.ones(df.shape[0], dtype='int')\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask = (mask==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we didn't get unlucky in our mask selection\n",
    "print \"% Good wines in Training:\", np.mean(df.target[mask])\n",
    "print \"% Good wines in Testing:\", np.mean(df.target[~mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% Good wines in Training: 0.136600625652\n",
    "% Good wines in Testing: 0.134375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clfTree1 = tree.DecisionTreeClassifier()\n",
    "\n",
    "parameters = {\"max_depth\": [1, 2, 3, 4, 5, 6, 7], 'min_samples_leaf': [1, 2, 3, 4, 5, 6]}\n",
    "clfTree1, Xtrain, ytrain, Xtest, ytest = do_classify(clfTree1, parameters, df, \n",
    "                                                     ['alcohol', 'fixed acidity'],'target', 1, \n",
    "                                                     mask=mask, n_jobs = 4, score_func = 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2tree(plt, Xtrain, Xtest, ytrain, ytest, clfTree1, \n",
    "           lab = ['alcohol', 'fixed acidity'], alpha = 1, plot_test = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2tree(plt, Xtrain, Xtest, ytrain, ytest, clfTree1, \n",
    "           lab = ['alcohol', 'fixed acidity'], alpha = 1, plot_train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clfTree_temp = tree.DecisionTreeClassifier()\n",
    "\n",
    "parameters = {\"max_depth\": [None], 'min_samples_leaf': [4, 5, 6]}\n",
    "clfTree_temp, Xtrain, ytrain, Xtest, ytest = do_classify(clfTree_temp, parameters, df, \n",
    "                                                     ['alcohol', 'fixed acidity'],'target', 1, \n",
    "                                                     mask=mask, n_jobs = 4, score_func = 'f1')\n",
    "plot_2tree(plt, Xtrain, Xtest, ytrain, ytest, clfTree_temp, \n",
    "           lab = ['alcohol', 'fixed acidity'], alpha = 1, plot_train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfTree2 = tree.DecisionTreeClassifier()\n",
    "\n",
    "parameters = {\"max_depth\": [1, 2, 3, 4, 5, 6, 7], 'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "clfTree2, Xtrain, ytrain, Xtest, ytest = do_classify(clfTree2, parameters, df, \n",
    "                                                     Xnames,'target', 1, \n",
    "                                                     mask=mask, n_jobs = 4, score_func = 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clfForest = RandomForestClassifier()\n",
    "\n",
    "parameters = {\"n_estimators\": range(1, 20)}\n",
    "clfForest, Xtrain, ytrain, Xtest, ytest = do_classify(clfForest, parameters, \n",
    "                                                       df, Xnames, 'target', 1, mask=mask, \n",
    "                                                       n_jobs = 4, score_func='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_list = clfForest.feature_importances_\n",
    "name_list = df.columns\n",
    "importance_list, name_list = zip(*sorted(zip(importance_list, name_list)))\n",
    "plt.barh(range(len(name_list)),importance_list,align='center')\n",
    "plt.yticks(range(len(name_list)),name_list)\n",
    "plt.xlabel('Relative Importance in the Random Forest')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Relative importance of Each Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
